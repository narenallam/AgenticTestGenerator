"""
Agent-level evaluations for the test generation system.

Evaluates:
- Planner: Task decomposition accuracy
- Coder: Test generation quality
- Critic: Review effectiveness
"""

from typing import Dict, List, Optional

from ..base import BaseEvaluator, EvalLevel, EvalResult, generate_eval_id


# ═══════════════════════════════════════════════════════════════════════════
# Planner Evaluator
# ═══════════════════════════════════════════════════════════════════════════


class PlannerEvaluator(BaseEvaluator):
    """Evaluate Planner agent performance."""
    
    def __init__(self):
        """Initialize Planner evaluator."""
        super().__init__(name="planner_eval", level=EvalLevel.AGENT)
    
    def evaluate(
        self,
        generated_plan: Dict,
        source_code: str,
        language: str = "python",
        eval_id: Optional[str] = None,
    ) -> EvalResult:
        """
        Evaluate Planner's task decomposition.
        
        Args:
            generated_plan: Plan generated by Planner agent
            source_code: Source code to generate tests for
            language: Programming language
            eval_id: Optional evaluation ID
        
        Returns:
            EvalResult with Planner metrics
        """
        if eval_id is None:
            eval_id = generate_eval_id("planner")
        
        result = self.create_result(eval_id)
        result.metadata = {
            "language": language,
            "plan": generated_plan,
        }
        
        try:
            # Metric 1: Plan completeness (0-1)
            completeness = self._evaluate_completeness(generated_plan, source_code)
            result.add_metric(
                name="completeness",
                value=completeness,
                unit="ratio",
                description="Plan includes all necessary steps",
                threshold=0.8,
            )
            
            # Metric 2: Tool selection accuracy (0-1)
            tool_accuracy = self._evaluate_tool_selection(generated_plan)
            result.add_metric(
                name="tool_accuracy",
                value=tool_accuracy,
                unit="ratio",
                description="Correct tools selected for tasks",
                threshold=0.9,
            )
            
            # Metric 3: Iteration efficiency (0-1)
            efficiency = self._evaluate_efficiency(generated_plan)
            result.add_metric(
                name="efficiency",
                value=efficiency,
                unit="ratio",
                description="Plan is efficient (minimal steps)",
                threshold=0.7,
            )
            
            # Metric 4: Goal alignment (0-1)
            # Check if plan aims for 90% coverage and 90% pass rate
            goal_alignment = self._evaluate_goal_alignment(generated_plan)
            result.add_metric(
                name="goal_alignment",
                value=goal_alignment,
                unit="ratio",
                description="Plan aligns with 90/90 goals",
                threshold=0.8,
            )
            
            # Calculate overall score
            weights = {
                "completeness": 0.30,
                "tool_accuracy": 0.30,
                "efficiency": 0.20,
                "goal_alignment": 0.20,
            }
            result.calculate_score(weights=weights)
            
            # Recommendations
            result.recommendations.extend(self._generate_recommendations(result))
            
            result.mark_completed()
        
        except Exception as e:
            result.mark_failed(str(e))
        
        return result
    
    def _evaluate_completeness(self, plan: Dict, source_code: str) -> float:
        """Evaluate if plan includes all necessary steps."""
        score = 0.0
        
        # Required steps
        required_steps = [
            "analyze",     # Analyze source code
            "search",      # Search for context
            "generate",    # Generate tests
            "execute",     # Run tests
            "refine",      # Refine based on results
        ]
        
        plan_text = str(plan).lower()
        
        for step in required_steps:
            if step in plan_text:
                score += 0.2
        
        return min(score, 1.0)
    
    def _evaluate_tool_selection(self, plan: Dict) -> float:
        """Evaluate if correct tools are selected."""
        score = 1.0
        
        # Check for appropriate tool usage
        tools = plan.get("tools", [])
        if isinstance(tools, list):
            # Expected tools for test generation
            expected_tools = {
                "search_code", "get_code_context",
                "generate_tests", "execute_tests"
            }
            
            plan_tools = set(tools)
            
            # Reward for using expected tools
            overlap = expected_tools & plan_tools
            score = len(overlap) / len(expected_tools) if expected_tools else 1.0
        
        return score
    
    def _evaluate_efficiency(self, plan: Dict) -> float:
        """Evaluate plan efficiency (not too many steps)."""
        steps = plan.get("steps", [])
        if not isinstance(steps, list):
            steps = []
        
        num_steps = len(steps)
        
        # Optimal: 5-8 steps
        # Too few (<3) or too many (>12) = lower score
        if 5 <= num_steps <= 8:
            return 1.0
        elif 3 <= num_steps < 5 or 8 < num_steps <= 10:
            return 0.8
        elif num_steps < 3 or 10 < num_steps <= 12:
            return 0.6
        else:
            return 0.4
    
    def _evaluate_goal_alignment(self, plan: Dict) -> float:
        """Evaluate if plan explicitly targets 90/90 goals."""
        plan_text = str(plan).lower()
        
        score = 0.0
        
        # Check for coverage mentions
        if "coverage" in plan_text:
            score += 0.3
            if "90" in plan_text or "0.9" in plan_text:
                score += 0.2
        
        # Check for test execution/validation
        if any(word in plan_text for word in ["execute", "run", "validate"]):
            score += 0.3
        
        # Check for iterative refinement
        if any(word in plan_text for word in ["refine", "improve", "iterate"]):
            score += 0.2
        
        return min(score, 1.0)
    
    def _generate_recommendations(self, result: EvalResult) -> List[str]:
        """Generate recommendations for Planner improvement."""
        recommendations = []
        
        for name, metric in result.metrics.items():
            if not metric.passed:
                if name == "completeness":
                    recommendations.append("⚠️ Plan should include: analyze, search, generate, execute, refine")
                elif name == "tool_accuracy":
                    recommendations.append("⚠️ Use appropriate tools: search_code, generate_tests, execute_tests")
                elif name == "efficiency":
                    recommendations.append("⚠️ Optimize plan to 5-8 steps for efficiency")
                elif name == "goal_alignment":
                    recommendations.append("⚠️ Plan should explicitly target 90% coverage and 90% pass rate")
        
        if not recommendations:
            recommendations.append("✅ Planner performance is excellent!")
        
        return recommendations


# ═══════════════════════════════════════════════════════════════════════════
# Coder Evaluator
# ═══════════════════════════════════════════════════════════════════════════


class CoderEvaluator(BaseEvaluator):
    """Evaluate Coder/Test Generator agent performance."""
    
    def __init__(self):
        """Initialize Coder evaluator."""
        super().__init__(name="coder_eval", level=EvalLevel.AGENT)
    
    def evaluate(
        self,
        generated_tests: str,
        source_code: str,
        language: str = "python",
        coverage: Optional[float] = None,
        pass_rate: Optional[float] = None,
        eval_id: Optional[str] = None,
    ) -> EvalResult:
        """
        Evaluate Coder's test generation quality.
        
        Args:
            generated_tests: Tests generated by Coder agent
            source_code: Source code being tested
            language: Programming language
            coverage: Actual coverage achieved (if known)
            pass_rate: Actual pass rate (if known)
            eval_id: Optional evaluation ID
        
        Returns:
            EvalResult with Coder metrics
        """
        if eval_id is None:
            eval_id = generate_eval_id("coder")
        
        result = self.create_result(eval_id)
        result.metadata = {
            "language": language,
            "test_length": len(generated_tests),
        }
        
        try:
            # Metric 1: Syntax correctness (0-1)
            syntax_score = self._evaluate_syntax(generated_tests, language)
            result.add_metric(
                name="syntax_correctness",
                value=syntax_score,
                unit="ratio",
                description="Generated tests are syntactically valid",
                threshold=1.0,
            )
            
            # Metric 2: Framework usage (0-1)
            framework_score = self._evaluate_framework_usage(generated_tests, language)
            result.add_metric(
                name="framework_usage",
                value=framework_score,
                unit="ratio",
                description="Correct test framework conventions",
                threshold=0.9,
            )
            
            # Metric 3: Coverage goal (0-1)
            if coverage is not None:
                coverage_goal_score = min(coverage / 0.90, 1.0)
            else:
                coverage_goal_score = 0.5  # Unknown
            
            result.add_metric(
                name="coverage_goal",
                value=coverage_goal_score,
                unit="ratio",
                description="Achieves 90% coverage goal",
                threshold=1.0,
            )
            
            # Metric 4: Pass rate goal (0-1)
            if pass_rate is not None:
                pass_rate_goal_score = min(pass_rate / 0.90, 1.0)
            else:
                pass_rate_goal_score = 0.5  # Unknown
            
            result.add_metric(
                name="pass_rate_goal",
                value=pass_rate_goal_score,
                unit="ratio",
                description="Achieves 90% pass rate goal",
                threshold=1.0,
            )
            
            # Metric 5: Code quality (0-1)
            quality_score = self._evaluate_code_quality(generated_tests)
            result.add_metric(
                name="code_quality",
                value=quality_score,
                unit="ratio",
                description="Clean, readable, well-structured code",
                threshold=0.8,
            )
            
            # Calculate overall score
            weights = {
                "syntax_correctness": 0.25,
                "framework_usage": 0.15,
                "coverage_goal": 0.25,
                "pass_rate_goal": 0.25,
                "code_quality": 0.10,
            }
            result.calculate_score(weights=weights)
            
            # Recommendations
            result.recommendations.extend(self._generate_recommendations(result, coverage, pass_rate))
            
            result.mark_completed()
        
        except Exception as e:
            result.mark_failed(str(e))
        
        return result
    
    def _evaluate_syntax(self, test_code: str, language: str) -> float:
        """Check if generated tests are syntactically valid."""
        try:
            if language == "python":
                import ast
                ast.parse(test_code)
                return 1.0
            elif language == "java":
                # Simplified: check for basic syntax patterns
                if "class" in test_code and "test" in test_code.lower():
                    return 1.0
                return 0.5
            elif language in ["javascript", "typescript"]:
                # Simplified: check for test patterns
                if ("test(" in test_code or "it(" in test_code) and "expect(" in test_code:
                    return 1.0
                return 0.5
        except SyntaxError:
            return 0.0
        except Exception:
            return 0.5
        
        return 0.5
    
    def _evaluate_framework_usage(self, test_code: str, language: str) -> float:
        """Check if correct test framework is used."""
        test_code_lower = test_code.lower()
        score = 0.0
        
        if language == "python":
            # Check for pytest or unittest
            if "pytest" in test_code_lower or "import pytest" in test_code_lower:
                score += 0.5
            if "def test_" in test_code_lower:
                score += 0.5
        elif language == "java":
            # Check for JUnit
            if "@test" in test_code_lower and "import org.junit" in test_code_lower:
                score = 1.0
        elif language in ["javascript", "typescript"]:
            # Check for Jest/Mocha
            if ("describe(" in test_code or "test(" in test_code) and "expect(" in test_code:
                score = 1.0
        
        return min(score, 1.0)
    
    def _evaluate_code_quality(self, test_code: str) -> float:
        """Evaluate code quality (readability, structure)."""
        score = 1.0
        
        # Check for documentation
        if '"""' in test_code or "'''" in test_code or "/**" in test_code:
            score += 0.2
        
        # Check for clear test names
        if "test_" in test_code or "should_" in test_code:
            score += 0.2
        
        # Penalize very long lines (>100 chars)
        lines = test_code.split('\n')
        long_lines = sum(1 for line in lines if len(line) > 100)
        if long_lines > 0:
            score -= 0.1 * (long_lines / len(lines))
        
        return max(0.0, min(score, 1.0))
    
    def _generate_recommendations(
        self,
        result: EvalResult,
        coverage: Optional[float],
        pass_rate: Optional[float]
    ) -> List[str]:
        """Generate recommendations for Coder improvement."""
        recommendations = []
        
        for name, metric in result.metrics.items():
            if not metric.passed:
                if name == "syntax_correctness":
                    recommendations.append("❌ CRITICAL: Fix syntax errors in generated tests")
                elif name == "framework_usage":
                    recommendations.append("⚠️ Use correct test framework conventions")
                elif name == "coverage_goal":
                    if coverage is not None:
                        gap = (0.90 - coverage) * 100
                        recommendations.append(f"⚠️ Coverage is {coverage*100:.1f}%, need {gap:.1f}% more to reach 90% goal")
                    else:
                        recommendations.append("⚠️ Measure and improve coverage to reach 90% goal")
                elif name == "pass_rate_goal":
                    if pass_rate is not None:
                        gap = (0.90 - pass_rate) * 100
                        recommendations.append(f"⚠️ Pass rate is {pass_rate*100:.1f}%, need {gap:.1f}% more to reach 90% goal")
                    else:
                        recommendations.append("⚠️ Improve test reliability to reach 90% pass rate goal")
                elif name == "code_quality":
                    recommendations.append("⚠️ Improve code quality: add docs, shorten lines, use clear names")
        
        if not recommendations:
            recommendations.append("✅ Coder performance is excellent!")
        
        return recommendations


# ═══════════════════════════════════════════════════════════════════════════
# Critic Evaluator
# ═══════════════════════════════════════════════════════════════════════════


class CriticEvaluator(BaseEvaluator):
    """Evaluate Critic agent performance."""
    
    def __init__(self):
        """Initialize Critic evaluator."""
        super().__init__(name="critic_eval", level=EvalLevel.AGENT)
    
    def evaluate(
        self,
        critic_review: Dict,
        test_code: str,
        known_issues: Optional[List[str]] = None,
        eval_id: Optional[str] = None,
    ) -> EvalResult:
        """
        Evaluate Critic's review effectiveness.
        
        Args:
            critic_review: Review generated by Critic agent
            test_code: Test code being reviewed
            known_issues: Known issues in the test code (for validation)
            eval_id: Optional evaluation ID
        
        Returns:
            EvalResult with Critic metrics
        """
        if eval_id is None:
            eval_id = generate_eval_id("critic")
        
        result = self.create_result(eval_id)
        result.metadata = {
            "review": critic_review,
            "known_issues": known_issues or [],
        }
        
        try:
            # Metric 1: Issue detection accuracy (0-1)
            detection_accuracy = self._evaluate_detection_accuracy(
                critic_review, test_code, known_issues
            )
            result.add_metric(
                name="detection_accuracy",
                value=detection_accuracy,
                unit="ratio",
                description="Correctly identifies real issues",
                threshold=0.8,
            )
            
            # Metric 2: False positive rate (0-1, higher is better)
            false_positive_score = self._evaluate_false_positives(critic_review, test_code)
            result.add_metric(
                name="false_positive_rate",
                value=false_positive_score,
                unit="ratio",
                description="Low false alarms (higher is better)",
                threshold=0.8,
            )
            
            # Metric 3: Review completeness (0-1)
            completeness = self._evaluate_completeness(critic_review)
            result.add_metric(
                name="completeness",
                value=completeness,
                unit="ratio",
                description="Review covers all quality dimensions",
                threshold=0.8,
            )
            
            # Metric 4: Actionable feedback (0-1)
            actionable_score = self._evaluate_actionability(critic_review)
            result.add_metric(
                name="actionable_feedback",
                value=actionable_score,
                unit="ratio",
                description="Provides clear, actionable suggestions",
                threshold=0.8,
            )
            
            # Calculate overall score
            weights = {
                "detection_accuracy": 0.35,
                "false_positive_rate": 0.25,
                "completeness": 0.20,
                "actionable_feedback": 0.20,
            }
            result.calculate_score(weights=weights)
            
            # Recommendations
            result.recommendations.extend(self._generate_recommendations(result))
            
            result.mark_completed()
        
        except Exception as e:
            result.mark_failed(str(e))
        
        return result
    
    def _evaluate_detection_accuracy(
        self,
        review: Dict,
        test_code: str,
        known_issues: Optional[List[str]]
    ) -> float:
        """Evaluate if Critic catches known issues."""
        if not known_issues:
            return 0.8  # No ground truth, assume decent
        
        review_text = str(review).lower()
        
        caught = 0
        for issue in known_issues:
            if issue.lower() in review_text:
                caught += 1
        
        return caught / len(known_issues) if known_issues else 0.8
    
    def _evaluate_false_positives(self, review: Dict, test_code: str) -> float:
        """Estimate false positive rate (simplified)."""
        issues_found = review.get("issues", [])
        
        if not issues_found:
            return 1.0  # No false positives
        
        # Heuristic: if many issues but test is simple, likely false positives
        test_lines = len(test_code.split('\n'))
        num_issues = len(issues_found) if isinstance(issues_found, list) else 0
        
        if test_lines < 20 and num_issues > 5:
            return 0.6  # Likely too many false positives
        elif test_lines < 50 and num_issues > 10:
            return 0.7
        else:
            return 0.9  # Reasonable
    
    def _evaluate_completeness(self, review: Dict) -> float:
        """Check if review covers all quality dimensions."""
        dimensions = [
            "style", "quality", "correctness",
            "coverage", "determinism", "mocking"
        ]
        
        review_text = str(review).lower()
        
        covered = sum(1 for dim in dimensions if dim in review_text)
        
        return covered / len(dimensions)
    
    def _evaluate_actionability(self, review: Dict) -> float:
        """Check if feedback is actionable."""
        review_text = str(review).lower()
        
        score = 0.0
        
        # Check for specific suggestions
        actionable_indicators = [
            "should", "must", "add", "remove", "fix",
            "change", "improve", "use", "avoid"
        ]
        
        for indicator in actionable_indicators:
            if indicator in review_text:
                score += 0.15
        
        # Check for examples/code snippets
        if "```" in str(review) or "example" in review_text:
            score += 0.2
        
        return min(score, 1.0)
    
    def _generate_recommendations(self, result: EvalResult) -> List[str]:
        """Generate recommendations for Critic improvement."""
        recommendations = []
        
        for name, metric in result.metrics.items():
            if not metric.passed:
                if name == "detection_accuracy":
                    recommendations.append("⚠️ Improve issue detection: catch known problems")
                elif name == "false_positive_rate":
                    recommendations.append("⚠️ Reduce false positives: be more precise")
                elif name == "completeness":
                    recommendations.append("⚠️ Review all dimensions: style, quality, coverage, determinism")
                elif name == "actionable_feedback":
                    recommendations.append("⚠️ Provide specific, actionable suggestions with examples")
        
        if not recommendations:
            recommendations.append("✅ Critic performance is excellent!")
        
        return recommendations

